
## 1부
	
지난 시간까지 환경설정 쭉 진행을 했습니다.
이제 진짜 dag을 한번 생성해보도록 하겠습니다.
우선 airflow 폴더 아래에 dags 라는 폴더 하나 만들꼐요.
앞으로 dag들은 모두 이 폴더 안에다가 만들겁니다. 
그리고 새로운 파이썬 파일 만들기에서 파일명은 dags_bash_operator 라고 해보겠습니다. 

아무것도 없는 상태에서 DAG을 만드려면 막막하죠?
그래서 Airflow 샘플 DAG을 한번 보도록 하겠습니다. 
wsl 터미널 창을 여시구요, Airflow 서비스 안올리신 분들은 airflow 서비스 올려보도록 하겠습니다.
저는 이미 올라가 있구요.

airflow 페이지에 들어오면 샘플 코드들이 있는데 좀 밑에

example_bash_operator 이거 한번 보도록 할께요.
클릭하시고, 오른쪽 탭에 Code 버튼 눌러보겠습니다. 

위에서부터 코드를 하나씩 볼께요.
from하고 import 는 DAG을 만드는데 필요한 라이브러리 가져오는 부분이구요.
아까 저희도 airflow 관련 라이브러리를 모두 설치했기 떄문에 그대로 사용하실 수 있습니다. 

좀 더 밑에 보면 with DAG이라는 구문이 나오는데,
이건 DAG이라는 클래스로부터 인스턴스화, 즉 객체를 만드는 구문입니다. 
with 뒤에 나오는 대문자 DAG은 위에서 임포트해온 클래스를 의미하구요.
as 소문자 dag은 생성된 객체를 의미합니다. 

참고로 파이썬에서 with문을 쓰는건 자원 생성과 소멸을 한번에 관리하기 위한 문법이에요.
일단 이 with문을 복사해서 붙여넣어볼께요. 

with DAG(
    dag_id="example_bash_operator",
    schedule="0 0 * * *",
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=["example", "example2"],
    params={"example_key": "example_value"},
) as dag:


붙여보면 빨간줄이 뜨는게 보이는데, 필요한 라이브러리를 import 해오지 않았기 떄문이에요.
그래서 
from airflow import DAG
from airflow.operators.bash import BashOperator 
import pendulum

이 부분도 복사해서 가져오도록 하겠습니다. 
참고로 pendulum 이라는 라이브러리는 파이썬에 datetime 이라는 날짜 관련 데이터타입이 있는데 이 날짜 데이터타입을 쉽게 다룰 수 있도록 해주는 라이브러리에요.

DAG 클래스의 파라미터들을 하나씩 보겠습니다. 
먼저 dag_id는 우리가 airflow 화면에 들어왔을 때 보이는 dag 의 이름을 의미합니다.
주의해야할 점은 dag을 만드는 파이썬 파일명이 dag명이 되는 것이 아니라 dag_id 파라미터에다가 주는 값이 dag 명이 된다는 점 기억하셔야 합니다. 
파이썬 파일명은 아무이름을 줘도 상관없어요.

그런데 가급적 유지보수를 편하기 하려면 파이썬 파일명을 dag_id 와 일치시키는게 좋습니다. 
그래야 나중에 dag 수정할 떄 파이썬 파일을 찾아가기가 쉬워요.
제가 첫 수업때 airflow는 프로그래밍 표준같은게 잘 잡혀있어야 한다고 말씀드렸잖아요?

그런 표준들이 이런겁니다. dag_id와 파이썬 파일명은 일치시켜라.
dag_id의 네이밍룰은 어떻게 해라.
하는 것들이 모두 표준이에요.

그래서 저는 dag_id 도 dags_bash_operator 라고 지정할께요.
그 다음 schedule 인데요. 이 dag이 언제 수행될지를 cron 스케줄 형식으로 적어주면 됩니다.
다음 시간에 cron 스케줄에 대해서 설명드릴건데요.
우선은 이 형태는 매일 0분 0시 에 수행되도록한다. 고 이해하시면 되요.
만약 스케줄을 주기 싫고, 수작업으로만 돌리고 싶다면 
schedule=None으로 주면 됩니다. 

그 다음 start_date는 이 dag이 언제부터 수행되어야 하는지를 말하는데
밑에 있는 catchup 변수와 같이 봐야해요.
만약 오늘이 3월 1일이고 startdate가 2월 1일이면 한달간의 시간이 지난 상태잖아요?
이때 catchup 변수를 True로 주면 지나간 1달을 배치를 모두 수행하게 됩니다.
catchup을 False로 주면 지나간 날짜는 무시하고 내일부터 첫 수행하게 되요.
주의해야할 점은 catchup 을 True로 주면 지나간 날짜의 배치를 차례차례 수행하는게 아니라 한번에 수행하게 됩니다. 그러면 여러분들이 dag을 만든 방식에 따라 오류가 생길수도 있어요.
일단 저는 catchup을 False로 두겠습니다. 

그리고 타임존은 한국으로 지정할꼐요. tz='Asia/Seoul'로 하시면 됩니다.
만약 UTC로 지정하거나 tz='Asia/Seoul'을 넣지 않으면 기본 UTC로 지정됩니다.
한국 타임존은 UTC보다 9시간이 빨라요.
그래서 UTC 기준의 0시 0분은 한국 시간으로 9시 0분이 됩니다. 
그래서 9시 0분에 수행되기 때문에 꼭 타임존을 넣어주세요.

다음 dagrun_timeout은 dag의 타임아웃을 설정하는 값이고 현재 60분으로 설정돼있어요.
60분이 지나면 이 dag을 실패처리하겠다는 의미에요.
저는 이 파라미터는 쓰지 않을께요. 그냥 지우면 타임아웃 설정은 하지 않는 거에요. 

그리고 tag는 airflow 첫 페이지에서 dag들을 보면 dag 이름 아래에 파란색 박스로 라벨링같은게 있는데요. 이게 tag 입니다. 이 tag를 클릭하면 같은 tag들만 모아서 dag을 볼 수 있는 기능이 있습니다. 저는 bash 이라고 하나만 입력해놓을께요.
params는 dag 객체 밑에 task들을 입력을 할건데요, 모든 task에 공통적으로 할당할 파라미터들을 정의해주는 겁니다. 
필요없으면 이렇게 제거해주시면 됩니다.

여기까지 입력해주시면 DAG 클래스를 통해서 인스턴스화, 객체화가 된 dag 이라는게 만들어지구요.
다음부터는 사용할 task들을 정의해주시면 됩니다. 

어떻게 코드를 작성하는지 다시 한번 샘플 dag을 보도록 할께요.
보시면 howto_operator_bash 이라는 부분이 있습니다. 
이 부분이 bash operator를 이용해서 task를 만드는 방법을 설명한 코드에요. 
똑같이 복사해서 붙여오겠습니다. 
참고로 bash 오퍼레이터는 리눅스에서 쉘 스크립트 명령을 수행할 때 쓰는 오퍼레이터에요.
일단 한번 보겠습니다. 

    # [START howto_operator_bash]
    run_this = BashOperator(
        task_id="run_after_loop",
        bash_command="echo 1",
    )
    # [END howto_operator_bash]


위에서 DAG 클래스로 작업한 것과 비슷하게 BashOperator라는 클래스를 인스턴스화해서 run_this 라는 객체를 만든겁니다. 
Task_id 는 DAG에서 graph 눌렀을 때 뜨는 task들의 이름이에요. 
잘 보시면 run_this 라는 이름이 뜨는게 아니라 run_after_loop 라는 이름이 뜹니다. 
즉, DAG과 비슷하게
객체명이 task 명이 되는것이 아니라 task_id 값이 task 명이 되는거에요.
이번에도 객체명을 task명과 동일하게 만드는걸 표준으로 하겠습니다. 

우리는 bash_task_1 이라는 task 명을 쓸꼐요.
그래서 객체명과 task_id 명을 동일하게 부여하겠습니다. 
이렇게 해둬야 나중에 코드 유지보수하기에 쉽습니다.
참고로 task_id 는 띄어쓰기는 있으면 안됩니다. 꼭 문자열로 붙여서 써주세요.

그리고 bash_command는 실행시킬 쉘 명령을 입력하면 됩니다. 
저는 echo "who_am_i" 라고 해볼께요. echo는 프린트문과 비슷하다고 보시면 되요. 
그럼 최종적으로 이렇게 입력하면 됩니다. 

bash_task_1 = BashOperator(
        task_id="bash_task_1",
        bash_command='echo who_am_i'
    )

그리고 두 번째 task를 정의해볼께요.
이번에는 bash_task_2 라고 해볼게요.

    bash_task_2 = BashOperator(
        task_id='bash_task_2',
        bash_command='echo $HOSTNAME',
    )

두 번째 task에서 실행할 쉘 스크립트 명령은 echo $HOSTNAME입니다.
참고로 지금 떠있는 wsl에서 echo $HOSTNAME 쳐볼까요?

그럼 wsl 의 호스트명이 나옵니다.
동일한 쉘 명령을 bash operator를 통해서 하는거라고 보면 되요.

자, 이렇게 하고 두 task를 연결해주면 됩니다.
task를 연결하는건 꺽새로 이어줍니다.

bash_task_1 >> bask_task_2 이렇게 작성하면
bash_task_1이 돌고 bash_task_2를 수행해라. 라는 뜻이 됩니다.

자 여기까지 작성하면 다 작성한거에요.
이제 만든 dag을 push 하고 wsl에서 pull 해볼께요. 

(git commit / push )



## 2부


이제 wsl 에서 git pull 해볼꼐요. 

(git pull)
자 우리가 만든 dag이 잘 받아졌습니다.
그런데 이 dag을 정확하게는 HOME 밑의 dags 폴더에 넣어줘야 해요.
왜냐면 airflow 컨테이너들은 dags 폴더와 연결되어있고, dags 폴더에 dag을 넣어줘야 인식할 수 있다고 했죠?
그럼 git pull 받은 dag을 dags에다가 다시 카피해서 보내줄까요? 
그렇게 해도 되는데, 매번 그렇게하면 너무 번거로워져요.

차라리 airflow 밑에 dags 폴더를 컨테이너가 바라볼 수 있도록, 즉 마운트할 수 있도록 바꿔주면 됩니다. 

한번 해볼께요.
HOME 밑의 docker-compose.yaml 파일을 여시고 위에서부터 좀 내려보시면 volumes 항목이 있습니다.
이 항목이 서로 연결해줄 wsl의 디렉토리와 컨테이너의 디렉토리를 지정하는 항목입니다. 
$ 표시되어 있는 부분은 쉘 스크립트의 문법인데, AIRFLOW_PROJ_DIR 에 값이 있으면 그 값을 쓰고 없으면 .을 표시해라 라는 문법입니다. 

한번 확인해볼까요?
wsl 창 하나 더 여시고 echo ${AIRFLOW_PROJ_DIR:-.}  입력해볼께요.
그럼 . 이 나오죠? AIRFLOW_PROJ_DIR 변수에 값이 없기 때문에 그렇습니다.
AIRFLOW_PROJ_DIR에 값을 넣고 다시 해볼까요?
AIRLFOW_PROJ_DIR=ABC 라고 임의 값을 넣고 다시 해볼꼐요.
다시  echo ${AIRFLOW_PROJ_DIR:-.}  해보면 이번엔 ABC가 나오죠?

그럼 $ 부분의 의미를 이해하셨을 거에요.
다시 .yaml 파일로 가서 해석해보면 ./dags를 컨테이너의 /opt/airflow/dags 디렉토리와 연결해라 라는 의미가 됩니다. 
./dags는 어디일까요? 바로 yaml 파일이 위치하고 있는 현재 디렉토리입니다. 
그러니까 홈 디렉토리의 dags 폴더를 말하는거에요.

그럼 우리가 원하는건 git pull 했을 때 dag이 저장되는 위치가 airflow 밑에 dags 이므로 
airflow/dags로 바꿔주면 되지 않을까요?
한번 수정해볼께요.

${AIRFLOW_PROJ_DIR:-.}/airflow/dags 이렇게 수정해보겠습니다. 

그러고 esc + 콜론 누르고 wq 눌러서 저장하고 나올께요.
반영된걸로 다시 시작하려면 docker compose 재시작하면 되요.
sudo docker compose down 하시고
sudo docker compose up 해볼꼐요.

(작업 후)

이제 airflow 페이지에 가서 dag이 잘 떴나 한번 볼께요.
localhost:8080 입력하시고 로그인 다시 해주셔야 해요.

자 여기 떴네요.
만약 dag 작성중에 오타가 있었거나 airflow가 dag을 파싱하는데 문제가 있으면
제일 맨 위 airflow 에 빨간색으로 표시가 떠요.
그 표시를 눌러보면 어디가 문제인지 표시가 될거에요.

이제 dag에 들어가서 오른쪽에 code 눌러볼께요.
우리가 작성한 내용 잘 들어와있구요.
이제 시작해보겠습니다. 

잘 수행되면 지금처럼 초록색으로 뜨구요. 수행하고 수행 결과를 보려면 Grid로 가시면 되요.
Grid는 스케줄 수행이력들을 전부 확인할 수 있는 탭이에요.

가장 먼저 왼쪽 위에 기다린 네모를 누르면 해당 dag에 대한 설명이 나옵니다. 
그리고 수행 결과를 확인하려면 각각의 task의 네모를 누르시면 되요.
먼저 bash_task_1을 볼까요?

로그는 가운데에 Log를 누르시면 됩니다. 
Log 윗부분은 보통 볼 내용이 잘 없고 수행 결과는 밑에 나와요.
수행 결과 who_am_i 라고 출력문이 잘 나왔네요.

그 다음 bash_task_2 를 볼까요?
뒤로 가서 bash_task_2 네모를 누르고 Log를 누르면
이번에는 어떤 값들이 떠있는걸 볼 수 있습니다. 

밑에 보면 이번에는 어떤 값이 출력돼있네요.
HOSTNAME 값이 이거인데, 도대체 어떤 호스트네임인걸까요? 

잠깐 퀴즈 드리겠습니다.
우리가 bash operator로 쉘 명령을 수행했는데 
이 쉘 명령을 수행한 주체는 누구일까요?

1번. WSL 컴퓨터
2번. airflow 컨테이너

정답은 2번입니다.
airflow 컨테이너에서 쉘 명령을 수행한거에요.
그럼 airflow 컨테이너 6개가 있는데 그 중 어떤 컨테이너가 쉘 명령을 수행했을까요?
우선 wsl에서 sudo docker ps 쳐볼께요.
그럼 6개의 컨테이너가 나오죠?

두 번째 퀴즈.
이 6개의 컨테이너 중에서 어느 컨테이너가 수행했을까요?
정답은 worker 라고 돼있는 컨테이너입니다.
Airflow는 가장 중요한 2개의 모듈이 있는데 하나는 스케줄러고 하나는 워커에요.
그 중 워커는 실제 task를 처리하는 역할을 합니다.

이 worker 컨테이너 안에 들어가보겠습니다.

sudo docker exec -it ID bash 쳐서 들어가볼께요.
그런데 지금 컨테이너 ID가 익숙하죠? 아까 airflow log에서 나왔던 HOSTNAME 이네요.
확인사살을 해보겠습니다.

지금 들어온 컨테이너 상에서 echo $HOSTNAME 쳐볼께요.
그럼 같은 값이 나오네요.
즉 우리가 bash operator을 통해서 쉘 명령을 수행한 것은
이 worker 노드에 들어와서 echo $HOSTNAME 한것과 같은거에요.

이걸 잘 아셔야해요.
airflow에서 실제 task를 처리하는 것은 worker 노드이다.

정리해보겠습니다. 
이번 시간에 많은 걸 했는데요.
첫 번째 bash operator를 사용했고 bash 오퍼레이터는 쉘 명령을 수행할 수 있는 오퍼레이터다. 
그리고 두 번째, git pull 한 dag을 에어플로우가 바로 인식할 수 있도록 .yaml 파일에서 volumes 항목을 수정했습니다. 
그리고 세 번째, task를 처리하는 노드는 worker 노드이다.

이렇게 정리할 수 있습니다.
다음 시간에는 cron 스케줄 작성법에 대해서 한번 보도록 하겠습니다. 

수고하셨습니다. 
